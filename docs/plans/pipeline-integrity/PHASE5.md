# Phase 5: AC Validation

## Overview

| Attribute | Value |
|-----------|-------|
| **Goal** | Validate acceptance criteria post-execution using heuristic matching |
| **Gaps addressed** | G3 (acceptance criteria never validated post-execution) |
| **Dependencies** | **Phase 3B required** -- integrates into `run_validator.py` end-of-run validation |
| **Estimated sessions** | 2 |
| **Risk level** | LOW -- non-blocking, informational only |
| **Produces for downstream** | Coverage metric in validation details; confidence signal for run quality |

## Pre-existing State (after Phase 3)

- `worker_pool/run_validator.py` exists (~200 lines) with `RunValidator` class
- `RunValidator.validate_run()` has a `_run_ac_validation()` method (currently a no-op placeholder)
- `RunValidationResult` dataclass has `ac_validation_summary: str` field
- `execution_runs` table has `validation_details TEXT` column storing JSON
- Acceptance criteria are stored in the `acceptance_criteria` column of the `tasks` table as text generated by decomposition Pass 3

## Task 5A: Heuristic AC Validator

### Problem

Acceptance criteria like "Loading a non-existent file raises ConfigNotFoundError" are generated in Pass 3, fed to RED/GREEN prompts as context, but never verified against actual code after execution. The system assumes that if VERIFY passes, the AC is met -- but VERIFY only checks "tests pass," not "the specific behavior described in the AC is tested."

### Solution

Parse structured AC text and match against code artifacts using heuristic patterns:

| AC Pattern | Detection Method |
|-----------|-----------------|
| Error handling ("raises X") | Check for `pytest.raises(X)` in tests + `raise X` in impl (AST) |
| Export ("exports X") | Check if `X` is defined in impl AST (function/class definition) |
| Import ("X importable") | Try `python -c 'from module import X'` |
| Endpoint ("responds to GET /path") | Check for route decorator in impl (AST) |
| GIVEN/WHEN/THEN | Match WHEN clause keywords against test function names/docstrings |
| Unrecognizable | Log as "unverifiable" with the literal AC text |

### Design Decisions

- **Non-blocking**: Results are informational. Stored in run validation details. AC validation failure does NOT fail the run.
- **Called during end-of-run validation** (Phase 3B) via the existing `_run_ac_validation()` hook in `RunValidator`. The validator iterates over all completed tasks, checks their AC, and aggregates results.
- **No LLM calls**: Pure heuristic. Zero cost, deterministic, no API key required.
- **Coverage metric**: Report "X of Y acceptance criteria verifiable, Z verified as satisfied." This gives a confidence signal:
  - High coverage + high satisfaction = strong confidence
  - Low coverage = AC text is too vague (improve decomposition prompts)
  - High coverage + low satisfaction = implementation gaps
- **Per-task granularity**: Results are per-task, per-AC. The aggregate summary goes into `validation_details` JSON.

### Implementation Details

**File: `src/tdd_orchestrator/worker_pool/ac_validator.py`** (new, ~200 lines)

```python
"""Heuristic validation of acceptance criteria against code artifacts."""
from __future__ import annotations

import ast
import logging
import re
from dataclasses import dataclass, field
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class ACResult:
    """Result of validating a single acceptance criterion."""
    criterion: str
    status: str  # "satisfied", "not_satisfied", "unverifiable"
    matcher: str  # Which matcher was used (or "none")
    detail: str


@dataclass
class TaskACResult:
    """Aggregate AC validation result for a single task."""
    task_key: str
    results: list[ACResult] = field(default_factory=list)

    @property
    def total(self) -> int:
        return len(self.results)

    @property
    def verifiable(self) -> int:
        return sum(1 for r in self.results if r.status != "unverifiable")

    @property
    def satisfied(self) -> int:
        return sum(1 for r in self.results if r.status == "satisfied")


@dataclass
class RunACResult:
    """Aggregate AC validation result for an entire run."""
    task_results: list[TaskACResult] = field(default_factory=list)

    @property
    def summary(self) -> str:
        total = sum(t.total for t in self.task_results)
        verifiable = sum(t.verifiable for t in self.task_results)
        satisfied = sum(t.satisfied for t in self.task_results)
        return (
            f"{verifiable}/{total} criteria verifiable, "
            f"{satisfied}/{verifiable} verified as satisfied"
            if verifiable > 0
            else f"0/{total} criteria verifiable"
        )


class ACValidator:
    """Validates acceptance criteria against code artifacts."""

    def __init__(self, base_dir: str) -> None:
        self._base_dir = Path(base_dir)

    async def validate_task_ac(
        self,
        task_key: str,
        acceptance_criteria: str,
        impl_file: str,
        test_file: str,
    ) -> TaskACResult:
        """Validate all acceptance criteria for a single task."""
        result = TaskACResult(task_key=task_key)
        criteria = self._parse_criteria(acceptance_criteria)

        for criterion in criteria:
            ac_result = await self._validate_single(
                criterion, impl_file, test_file
            )
            result.results.append(ac_result)

        return result

    async def validate_run_ac(
        self,
        tasks: list[dict[str, str]],
    ) -> RunACResult:
        """Validate AC for all tasks in a run."""
        run_result = RunACResult()
        for task in tasks:
            task_result = await self.validate_task_ac(
                task_key=task.get("key", "?"),
                acceptance_criteria=task.get("acceptance_criteria", ""),
                impl_file=task.get("impl_file", ""),
                test_file=task.get("test_file", ""),
            )
            run_result.task_results.append(task_result)
        return run_result

    def _parse_criteria(self, raw: str) -> list[str]:
        """Split acceptance criteria text into individual criteria.

        Handles:
        - Newline-separated criteria
        - Numbered lists ("1. criterion")
        - Bullet lists ("- criterion", "* criterion")
        - GIVEN/WHEN/THEN blocks (keep as single criterion)
        """
        ...

    async def _validate_single(
        self,
        criterion: str,
        impl_file: str,
        test_file: str,
    ) -> ACResult:
        """Validate a single acceptance criterion."""
        lower = criterion.lower().strip()

        # Try matchers in priority order
        if result := self._match_error_handling(criterion, impl_file, test_file):
            return result
        if result := self._match_export(criterion, impl_file):
            return result
        if result := self._match_import(criterion, impl_file):
            return result
        if result := self._match_endpoint(criterion, impl_file):
            return result
        if result := self._match_given_when_then(criterion, test_file):
            return result

        return ACResult(
            criterion=criterion,
            status="unverifiable",
            matcher="none",
            detail="No heuristic matcher for this criterion",
        )

    def _match_error_handling(
        self, criterion: str, impl_file: str, test_file: str
    ) -> ACResult | None:
        """Match 'raises X' / 'should raise X' patterns."""
        # Parse exception name from criterion
        # Check impl AST for 'raise ExceptionName'
        # Check test AST for 'pytest.raises(ExceptionName)'
        ...

    def _match_export(self, criterion: str, impl_file: str) -> ACResult | None:
        """Match 'exports X' / 'provides X' patterns."""
        # Parse symbol name from criterion
        # Check impl AST for function/class definition
        ...

    def _match_import(self, criterion: str, impl_file: str) -> ACResult | None:
        """Match 'importable' / 'can import' patterns."""
        # Parse module/symbol from criterion
        # Check if impl file exists and contains the symbol
        ...

    def _match_endpoint(self, criterion: str, impl_file: str) -> ACResult | None:
        """Match 'responds to GET /path' / 'endpoint' patterns."""
        # Parse HTTP method and path from criterion
        # Check impl AST for route decorator (@app.get, @router.post, etc.)
        ...

    def _match_given_when_then(
        self, criterion: str, test_file: str
    ) -> ACResult | None:
        """Match GIVEN/WHEN/THEN structured criteria."""
        # Extract WHEN clause keywords
        # Check test function names/docstrings for keyword matches
        ...
```

**File: `src/tdd_orchestrator/worker_pool/run_validator.py`** (~200 -> ~215 lines)

Replace the `_run_ac_validation` no-op:

```python
async def _run_ac_validation(
    self, run_id: str, result: RunValidationResult
) -> None:
    """Validate acceptance criteria for all completed tasks."""
    from tdd_orchestrator.worker_pool.ac_validator import ACValidator

    tasks = await self._get_completed_tasks(run_id)
    validator = ACValidator(self._base_dir)
    ac_result = await validator.validate_run_ac(tasks)
    result.ac_validation_summary = ac_result.summary
    logger.info("AC validation: %s", ac_result.summary)
```

### Test Cases

**File: `tests/unit/worker_pool/test_ac_validator.py`** (new, ~120 lines)

```python
# Parsing tests:
# Test: newline-separated criteria -> list of criteria
# Test: numbered list ("1. X\n2. Y") -> [X, Y]
# Test: bullet list ("- X\n- Y") -> [X, Y]
# Test: GIVEN/WHEN/THEN block -> single criterion
# Test: empty string -> empty list
# Test: single criterion (no separators) -> [criterion]

# Error handling matcher:
# Test: "raises ConfigNotFoundError" + impl has raise + test has pytest.raises -> satisfied
# Test: "raises ConfigNotFoundError" + impl missing raise -> not_satisfied
# Test: "raises ConfigNotFoundError" + test missing pytest.raises -> not_satisfied

# Export matcher:
# Test: "exports load_config function" + impl has def load_config -> satisfied
# Test: "exports ConfigLoader class" + impl has class ConfigLoader -> satisfied
# Test: "exports nonexistent" + impl missing -> not_satisfied

# Import matcher:
# Test: "module is importable" + impl file exists -> satisfied
# Test: "module is importable" + impl file missing -> not_satisfied

# Endpoint matcher:
# Test: "responds to GET /health" + impl has @app.get("/health") -> satisfied
# Test: "responds to POST /tasks" + impl has @router.post("/tasks") -> satisfied
# Test: "responds to GET /health" + no route decorator -> not_satisfied

# GIVEN/WHEN/THEN matcher:
# Test: "WHEN loading config THEN returns dict" + test_load_config exists -> satisfied
# Test: WHEN keywords not in any test function name -> not_satisfied

# Unverifiable:
# Test: "performance is acceptable" -> unverifiable
# Test: "user experience is smooth" -> unverifiable

# Aggregate:
# Test: RunACResult.summary format is correct
# Test: mixed results (some satisfied, some not, some unverifiable)
# Test: all unverifiable -> "0/N criteria verifiable"

# Integration:
# Test: validate_run_ac with multiple tasks -> aggregated results
```

### Files Changed

| File | Current | Delta | Projected |
|------|---------|-------|-----------|
| NEW: `worker_pool/ac_validator.py` | 0 | ~200 | ~200 |
| `worker_pool/run_validator.py` | ~200 | +15 | ~215 |
| NEW: `tests/unit/worker_pool/test_ac_validator.py` | 0 | ~120 | ~120 |

---

## Session Breakdown

### Session 1: AC Validator Implementation + Unit Tests

**Steps**:
1. Read `run_validator.py` to understand the `_run_ac_validation()` hook
2. Read sample `acceptance_criteria` from DB or test fixtures to understand real-world patterns
3. Create `ac_validator.py` with `ACValidator` class and all matchers
4. Write comprehensive unit tests for each matcher type
5. Verify: pytest + mypy + ruff

**Session boundary check**:
```bash
.venv/bin/pytest tests/unit/worker_pool/test_ac_validator.py -v
.venv/bin/mypy src/tdd_orchestrator/worker_pool/ac_validator.py --strict
.venv/bin/ruff check src/tdd_orchestrator/worker_pool/ac_validator.py
```

### Session 2: Integration with RunValidator + Full Regression

**Steps**:
1. Wire `ACValidator` into `RunValidator._run_ac_validation()`
2. Test end-to-end: run validation includes AC check results
3. Verify AC summary appears in `validation_details` JSON
4. Run full test suite regression
5. Verify run_validator.py stays under 300 lines

**Session boundary check**:
```bash
.venv/bin/pytest tests/unit/worker_pool/ -v
.venv/bin/pytest tests/ -v  # full regression
.venv/bin/mypy src/ --strict
.venv/bin/ruff check src/
```

---

## Verification Commands

```bash
# Unit tests for AC validator
.venv/bin/pytest tests/unit/worker_pool/test_ac_validator.py -v

# Full worker pool tests
.venv/bin/pytest tests/unit/worker_pool/ -v

# Full regression
.venv/bin/pytest tests/ -v

# Type checking
.venv/bin/mypy src/tdd_orchestrator/worker_pool/ --strict

# Linting
.venv/bin/ruff check src/tdd_orchestrator/worker_pool/
```

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Heuristic matchers miss valid implementations (false negatives) | "not_satisfied" when actually met | Non-blocking by design. Coverage metric shows what percentage is verifiable. |
| Heuristic matchers flag correct code (false positives) | "satisfied" when not actually met | Conservative matching (only claim "satisfied" with concrete evidence). Better to under-report than over-report. |
| AC text too vague for any matcher | Low coverage metric | Informational -- feeds back to improve decomposition Pass 3 prompt engineering. |
| AST parsing of impl/test files fails | Matcher returns unverifiable | Try/except around parse, degrade to "unverifiable" with reason logged. |
| Performance impact of reading/parsing many files | Slow run validation | AC validation runs once per run (not per task during execution). Acceptable latency for end-of-run check. |

---

## Integration Checklist (Post-Phase 5)

- [ ] `ac_validator.py` exists with all five matchers (error, export, import, endpoint, GIVEN/WHEN/THEN)
- [ ] Unrecognized criteria are reported as "unverifiable" (not silently dropped)
- [ ] `RunValidator._run_ac_validation()` calls `ACValidator`
- [ ] AC summary appears in `validation_details` JSON
- [ ] Coverage metric format: "X/Y criteria verifiable, Z/X verified as satisfied"
- [ ] AC validation is non-blocking (run passes even if AC validation finds issues)
- [ ] run_validator.py stays under 300 lines
- [ ] All existing tests pass (regression)
- [ ] mypy strict passes
- [ ] ruff check passes

---

## Dependency Tracking

### What Phase 5 Produces

| Output | Consumer |
|--------|----------|
| AC coverage metric | End-of-run validation details (stored in DB) |
| Per-task AC results | Future: decomposition feedback loop (improve Pass 3 prompts based on coverage data) |
| Pattern data on unverifiable AC | Future: add new matchers, or improve AC generation quality |

### What Phase 5 Consumes

| Input | Source |
|-------|--------|
| `run_validator.py` -- `_run_ac_validation()` hook | Phase 3B |
| `RunValidationResult.ac_validation_summary` field | Phase 3B |
| `validation_details` JSON column | Phase 3B |
| `acceptance_criteria` text from tasks table | Decomposition Pass 3 |
| `impl_file` and `test_file` paths from tasks table | Decomposition pipeline |

---

## Future Enhancements (Not in Scope)

These are documented for future consideration, not for Phase 5 implementation:

1. **LLM-based AC validation**: For unverifiable criteria, use an LLM to read the code and judge if the AC is met. High cost, non-deterministic, but would increase coverage dramatically. Gate behind a config flag and API key check.

2. **AC quality feedback**: Feed coverage metrics back to decomposition Pass 3. If a pattern of unverifiable AC emerges (e.g., vague performance criteria), adjust the prompt to generate more structured, machine-verifiable AC.

3. **AC -> test mapping**: Instead of heuristic matching, have the decomposer explicitly link each AC to a specific test function name. Then AC validation is a simple "does test_X exist and pass?" check. Requires decomposition pipeline changes.

4. **Promote to semi-blocking**: After data collection shows the matchers are reliable (low false positive rate), make AC validation semi-blocking: warn but don't fail if coverage is below a threshold (e.g., "fewer than 50% of AC are verifiable").
