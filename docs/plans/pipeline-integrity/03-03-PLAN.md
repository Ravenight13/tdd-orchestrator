# 03-03: End-of-Run Validator (3B, G5)

## Objective

Create `run_validator.py` that performs comprehensive validation after all phases complete. Replaces the placeholder `_run_end_of_run_validation()` in pool.py. Adds `validation_status` and `validation_details` columns to `execution_runs` table.

## Context

- @src/tdd_orchestrator/worker_pool/pool.py (~285 lines after 03-02 -- has `_run_end_of_run_validation` placeholder)
- @src/tdd_orchestrator/worker_pool/phase_gate.py (~200 lines after 03-02 -- test execution patterns to reuse)
- @src/tdd_orchestrator/subprocess_utils.py (~15 lines -- `resolve_tool()`)
- @src/tdd_orchestrator/database.py (need queries for all task files, orphaned tasks)
- @schema/schema.sql (748 lines -- execution_runs table at line 180)
- @src/tdd_orchestrator/worker_pool/done_criteria_checker.py (177 lines -- evaluate_criteria for aggregation)

## Tasks

### Task 1: Schema migration

**Type**: schema
**Files**:
- EDIT: `schema/schema.sql` (748 -> ~753 lines)
- EDIT: `src/tdd_orchestrator/database.py` (add migration + update query)

**Action**:
1. Add columns to `execution_runs` table definition:
   ```sql
   validation_status TEXT,        -- 'passed', 'failed', NULL (not yet validated)
   validation_details TEXT         -- JSON blob with per-check results
   ```
2. Add migration in `database.py` `initialize()` method (ALTER TABLE ADD COLUMN, safe for existing DBs)
3. Add `update_run_validation(run_id, status, details_json)` method to OrchestratorDB

**Verify**:
```bash
.venv/bin/pytest tests/unit/test_database.py -v
.venv/bin/mypy src/tdd_orchestrator/database.py --strict
```

**Done**: New columns exist. Migration is additive (no data loss).

### Task 2: Create run_validator.py

**Type**: feature (TDD)
**Files**:
- NEW: `tests/unit/worker_pool/test_run_validator.py` (~150 lines) -- write FIRST
- NEW: `src/tdd_orchestrator/worker_pool/run_validator.py` (~200 lines)

**Action**:

Write tests first, then implement:

**Dataclass `RunValidationResult`**:
- `passed: bool` (overall)
- `regression_passed: bool`
- `lint_passed: bool`
- `type_check_passed: bool`
- `import_check_passed: bool` (non-blocking)
- `orphaned_tasks: list[str]`
- `done_criteria_summary: str`
- `ac_validation_summary: str` (Phase 5 hook, empty string initially)
- `errors: list[str]`
- `to_json() -> str`: Serialize for `validation_details` column

**Class `RunValidator`**:
- `__init__(db, base_dir)`: Store dependencies
- `validate_run(run_id) -> RunValidationResult`: Orchestrate all checks
- `_run_full_regression(run_id, result)`: pytest on ALL test files from ALL tasks
- `_run_lint_and_types(run_id, result)`: ruff check + mypy on ALL impl files
- `_check_orphaned_tasks(run_id, result)`: Query tasks still in blocked/pending
- `_check_module_imports(run_id, result)`: Try importing all `module_exports` values
- `_aggregate_done_criteria(run_id, result)`: Re-evaluate done_criteria for all completed tasks
- `_run_ac_validation(run_id, result)`: No-op placeholder for Phase 5A

**Blocking vs non-blocking**:
- `result.passed = regression_passed AND lint_passed AND type_check_passed AND no orphaned_tasks`
- `import_check_passed` is **non-blocking** (logged, not gating)
- `ac_validation_summary` is **non-blocking** (Phase 5 hook)

**Timeout**: Configurable via `RunValidator.__init__(timeout=600)`. Full regression can be slow.

**Tests**:
- All checks pass -> `passed=True`
- Regression failure -> `passed=False`, errors populated
- Lint failure -> `passed=False`
- Type check failure -> `passed=False`
- Orphaned tasks found -> `passed=False`, task keys listed
- Import check failure -> `passed=True` (non-blocking), `import_check_passed=False`
- `to_json()` produces valid JSON with all fields
- Empty run (no tasks) -> `passed=True`

**Verify**:
```bash
.venv/bin/pytest tests/unit/worker_pool/test_run_validator.py -v
.venv/bin/mypy src/tdd_orchestrator/worker_pool/run_validator.py --strict
```

**Done**: `RunValidator.validate_run()` performs all checks and returns structured result.

### Task 3: Wire run_validator into pool.py

**Type**: integration
**Files**:
- EDIT: `src/tdd_orchestrator/worker_pool/pool.py` (~285 -> ~300 lines)

**Action**:
1. Replace `_run_end_of_run_validation` placeholder:
   ```python
   async def _run_end_of_run_validation(self) -> bool:
       from .run_validator import RunValidator
       validator = RunValidator(self.db, self.base_dir)
       result = await validator.validate_run(self.run_id)
       await self.db.update_run_validation(
           self.run_id,
           "passed" if result.passed else "failed",
           result.to_json(),
       )
       logger.info("End-of-run validation: %s", "PASSED" if result.passed else "FAILED")
       return result.passed
   ```

**Verify**:
```bash
.venv/bin/pytest tests/unit/worker_pool/ -v
.venv/bin/mypy src/tdd_orchestrator/worker_pool/pool.py --strict
```

**Done**: End-of-run validation executes after final phase and stores results in DB.

## Verification

```bash
.venv/bin/pytest tests/unit/worker_pool/ -v
.venv/bin/pytest tests/unit/ -v  # broader regression
.venv/bin/mypy src/tdd_orchestrator/worker_pool/ --strict
.venv/bin/mypy src/tdd_orchestrator/database.py --strict
.venv/bin/ruff check src/
wc -l src/tdd_orchestrator/worker_pool/run_validator.py  # expect ~200
wc -l src/tdd_orchestrator/worker_pool/pool.py           # expect ~300
```

## Success Criteria

- [ ] `validation_status` and `validation_details` columns in `execution_runs`
- [ ] `run_validator.py` exists (~200 lines)
- [ ] Checks: full regression, lint, type check, orphaned tasks, imports, done_criteria
- [ ] Import check is non-blocking (logged only)
- [ ] AC validation hook exists as no-op (Phase 5 fills it)
- [ ] `to_json()` produces valid JSON
- [ ] Results stored in `execution_runs` table
- [ ] pool.py under 400 lines
- [ ] mypy strict clean, ruff clean
